#!/usr/bin/python

import sys
import argparse
import theano.tensor as T

from cgml.data import makeRandomClassificationData, makeRandomRegressionData
from cgml.optimizers import MSGD
from cgml.classifiers import LogRegClassifier,MultiLayerPerceptronClassifier
#from cgml.costs import negativeLogLikelihood,squaredLoss
from cgml.bench import trainTestBench
from cgml.argparse_actions import giveArgs
#import cgml.argparse_defaults as defaults

args = giveArgs()

outStream = sys.stdout

outStream.write('Using the following configuration:\n')
outStream.write(' --model ' + str(args.Model) + '\n')
outStream.write(' --cost  ' + str(args.Cost)  + '\n')

n = 20
n_in  = 5
n_out = 3
learnRate = 0.1

x = T.dmatrix('x')
y = T.ivector('y')

model = args.Model(
    x     = x,
    n_in  = n_in,
    n_out = n_out)

cost = args.Cost(model.output,y)

# Use mini-batch stochastic gradient descent to optimize parameters 
msgd_optimizer = MSGD(
    cost      = cost,
    params    = model.params,
    learnRate = learnRate)

# Define how to quantify error
if type(model) == MultiLayerPerceptronClassifier or type(model) == LogRegClassifier:
    
    # Some random data for classification
    x_train,y_train = makeRandomClassificationData(
        n     = n,
        n_in  = n_in,
        n_out = n_out)
    
else:

    # Some random data for regression
    x_train, y_train = makeRandomRegressionData(
        n = n,
        n_in = n_in,
        n_out = n_out)

# Start the test bench
trainTestBench(
    x         = x,
    y         = y,
    x_train   = x_train,
    y_train   = y_train,
    x_test    = x_train,
    y_test    = y_train,
    model     = model,
    cost      = cost,
    optimizer = msgd_optimizer,
    verbose   = True)














