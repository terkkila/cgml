#!/usr/bin/python

import sys
import argparse
import theano
import theano.tensor as T
import numpy as np
import yaml

from cgml.io import DataReader
from cgml.argparse_actions import giveArgs
from cgml.computational_graph import ComputationalGraph

# Print outputs here
outStream = sys.stdout

# Print info here
infStream = sys.stderr

# Parsing the arguments
args = giveArgs(log = infStream)

# Open a log if needed
#log = ( open(args.log,'w') if args.log else None )

if args.load:

    infStream.write('\nLoading model from file: ' + args.load + '\n')

    model = ComputationalGraph.loadFromFile(args.load)

elif args.cg:

    # Create the model
    model = ComputationalGraph(schema = yaml.load(open(args.cg,'r')),
                               log   = infStream,
                               supCostWeight = args.supCostWeight,
                               unsupCostWeight = args.unsupCostWeight,
                               seed = args.seed)

if args.recompileOnLoad:
    model.compile(log = infStream)

#if model.type not in ['autoencoder']:
infStream.write(str(model.predict.maker.fgraph.toposort())+'\n')

# Check if GPU is being used or not
if np.any([isinstance(x.op, T.Elemwise) for x in model.predict.maker.fgraph.toposort()]):
    infStream.write('No GPU found -- using CPU instead\n')
else:
    infStream.write('Found GPU -- using that whenever possible\n')


# Write description of the model to infStream
infStream.write('\n' + str(model) + '\n')

if args.validData:

    infStream.write("Caching validation data for monitoring\n")

    xValid,yValid = DataReader(args.validData,
                               targetType = model.targetType,
                               delimiter = args.delimiter).cache()

else:

    xValid,yValid = None,None


infStream.write('\n')

if args.trainData:

    infStream.write("Starting to read input data for training\n")

    # Data reader for training
    drTrain = DataReader(args.trainData,
                         batchSize = args.deviceBatchSize,
                         targetType = model.targetType,
                         delimiter = args.delimiter)
     
    model.train(drTrain = drTrain,
                x_valid = xValid,
                y_valid = yValid,
                nPasses = args.nPasses,
                deviceBatchSize = args.deviceBatchSize,
                miniBatchSize = args.miniBatchSize,
                verbose = args.verbose,
                infStream = infStream)

infStream.write('\n')

if args.save:
    
    infStream.write('Saving model to file: ' + args.save + '\n')

    model.saveToFile(args.save)

def strVec(vec):
    if type(vec) in [np.int64,int,np.float64,float]:
        return str(vec)
    return ' '.join(map(str,vec))

def strMat(mat):
    return '\n'.join(strVec(map(strVec,row)) for row in mat)
    
if args.testData:

    infStream.write("Starting to read data for prediction\n")
    
    # Data reader for prediction
    drTest  = DataReader(args.testData,
                         batchSize = 1,
                         targetType = model.targetType,
                         delimiter = args.delimiter)
    
    # Start prediction
    for x_test,y_test in drTest:

        if model.type in ['classification','regression']:
            
            outStream.write( strMat(zip([y_test],[model.predict(x_test)])) + '\n' )
            
        elif model.type == 'autoencoder':
            
            outStream.write( strMat(zip([y_test],model.encode(x_test))) + '\n' )

        elif model.type == 'supervised-autoencoder':

            outStream.write( strMat(zip([y_test],
                                        model.predict(x_test),
                                        model.encode(x_test),
                                        model.decode(x_test))) + '\n' )








